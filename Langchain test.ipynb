{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-05T11:49:00.597062Z",
     "start_time": "2024-05-05T11:48:58.360829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 19:48:59,857 - modelscope - INFO - PyTorch version 2.2.1+cu121 Found.\n",
      "2024-05-05 19:48:59,859 - modelscope - INFO - Loading ast index from C:\\Users\\Weizh\\.cache\\modelscope\\ast_indexer\n",
      "2024-05-05 19:48:59,898 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 bc2963ceb052282714f0bb6917a5aaac and a total number of 972 components indexed\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "from modelscope import snapshot_download, Model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class baichuan2_LLM(LLM):\n",
    "    def __init__(self, model_path :str):\n",
    "        # model_path: Baichuan-7B-chat模型路径\n",
    "        # 从本地初始化模型\n",
    "        super().__init__()\n",
    "        print(\"正在从本地加载模型...\")\n",
    "        self.model = Model.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "        print(\"完成本地模型的加载\")\n",
    "\n",
    "    def _call(self, prompt : str, stop: Optional[List[str]] = None,\n",
    "              run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "              **kwargs: Any):\n",
    "        # 重写调用函数\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        # 重写调用函数\n",
    "        response= self.model(prompt)\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"baichuan2_LLM\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T11:49:00.601724Z",
     "start_time": "2024-05-05T11:49:00.598065Z"
    }
   },
   "id": "b24c5bcbee53e2ea",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 19:49:00,607 - modelscope - INFO - initialize model from Baichuan2-13B-Chat-4bits\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI\\Paipai AI chat\\venv\\Lib\\site-packages\\xformers\\__init__.py\", line 55, in _is_triton_available\n",
      "    from xformers.triton.softmax import softmax as triton_softmax  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI\\Paipai AI chat\\venv\\Lib\\site-packages\\xformers\\triton\\softmax.py\", line 11, in <module>\n",
      "    import triton\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 19:49:01,098 - modelscope - WARNING - ('MODELS', 'text-generation', 'Baichuan2-13B-Chat-4bits') not found in ast index file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin D:\\AI\\Paipai AI chat\\venv\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\"baichuan2_LLM\" object has no field \"model\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m llm \u001B[38;5;241m=\u001B[39m \u001B[43mbaichuan2_LLM\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mBaichuan2-13B-Chat-4bits\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 7\u001B[0m, in \u001B[0;36mbaichuan2_LLM.__init__\u001B[1;34m(self, model_path)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m正在从本地加载模型...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m \u001B[38;5;241m=\u001B[39m Model\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_path, device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m完成本地模型的加载\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\AI\\Paipai AI chat\\venv\\Lib\\site-packages\\pydantic\\v1\\main.py:357\u001B[0m, in \u001B[0;36mBaseModel.__setattr__\u001B[1;34m(self, name, value)\u001B[0m\n\u001B[0;32m    354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m object_setattr(\u001B[38;5;28mself\u001B[39m, name, value)\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__config__\u001B[38;5;241m.\u001B[39mextra \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m Extra\u001B[38;5;241m.\u001B[39mallow \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__fields__:\n\u001B[1;32m--> 357\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m object has no field \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__config__\u001B[38;5;241m.\u001B[39mallow_mutation \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__config__\u001B[38;5;241m.\u001B[39mfrozen:\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is immutable and does not support item assignment\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: \"baichuan2_LLM\" object has no field \"model\""
     ]
    }
   ],
   "source": [
    "llm = baichuan2_LLM(\"Baichuan2-13B-Chat-4bits\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T11:49:05.784842Z",
     "start_time": "2024-05-05T11:49:00.601724Z"
    }
   },
   "id": "b190ee0a0a6e7d77",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm('你是谁')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T11:48:08.182996Z",
     "start_time": "2024-05-05T11:48:08.181993Z"
    }
   },
   "id": "457e1b68911b8c0c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 20:00:27,715 - modelscope - INFO - PyTorch version 2.2.1+cu121 Found.\n",
      "2024-05-05 20:00:27,718 - modelscope - INFO - Loading ast index from C:\\Users\\Weizh\\.cache\\modelscope\\ast_indexer\n",
      "2024-05-05 20:00:27,776 - modelscope - INFO - Loading done! Current index file version is 1.13.1, with md5 bc2963ceb052282714f0bb6917a5aaac and a total number of 972 components indexed\n",
      "2024-05-05 20:00:29,065 - modelscope - INFO - initialize model from Baichuan2-13B-Chat-4bits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Baichuan2-13B-Chat-4bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\AI\\Paipai AI chat\\venv\\Lib\\site-packages\\xformers\\__init__.py\", line 55, in _is_triton_available\n",
      "    from xformers.triton.softmax import softmax as triton_softmax  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\AI\\Paipai AI chat\\venv\\Lib\\site-packages\\xformers\\triton\\softmax.py\", line 11, in <module>\n",
      "    import triton\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin D:\\AI\\Paipai AI chat\\venv\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 20:00:30,202 - modelscope - WARNING - ('MODELS', 'text-generation', 'Baichuan2-13B-Chat-4bits') not found in ast index file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Generating response for: Hello, how are you?\n",
      "Response generated successfully\n",
      "Generated Response: {'response': \"Hello! I'm great, thanks. How can I assist you today?\", 'history': [{'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': \"Hello! I'm great, thanks. How can I assist you today?\"}]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import Model  # Assuming this is the correct import for your model\n",
    "\n",
    "class LLMModelHandler:\n",
    "    def __init__(self, model_name: str):\n",
    "        print(\"Loading model:\", model_name)\n",
    "        self.model = Model.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "        # Assuming the model has its own tokenizer or equivalent processing method\n",
    "        print(\"Model loaded successfully\")\n",
    "\n",
    "    def generate_response(self, prompt: str):\n",
    "        print(\"Generating response for:\", prompt)\n",
    "        # 准备输入格式，假设这是单轮对话的开始\n",
    "        input_messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        # 直接调用模型的chat方法\n",
    "        response = self.model(input_messages)\n",
    "        return response['response']\n",
    "\n",
    "# Setup for testing\n",
    "model_name = \"Baichuan2-13B-Chat-4bits\"  # Your model name\n",
    "handler = LLMModelHandler(model_name)\n",
    "\n",
    "# Test example text\n",
    "test_prompt = \"Hello, how are you?\"\n",
    "response = handler.generate_response(test_prompt)\n",
    "print(\"Generated Response:\", response)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T12:00:48.625204Z",
     "start_time": "2024-05-05T12:00:23.913768Z"
    }
   },
   "id": "d6c90ae537524ee7",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "af819e74a20cd824"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
